2021-12-15 01:03:49,754 [INFO] ------------------------------------------------------------------------------------------------------------------------
2021-12-15 01:03:49,754 [INFO] Training models for smokers
2021-12-15 01:03:49,754 [INFO] ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
2021-12-15 01:03:49,755 [INFO] Starting training for smokers with learning augmentation.
2021-12-15 01:03:49,755 [INFO] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2021-12-15 01:03:49,755 [INFO] Starting training using the HuggingFace model distilbert-base-uncased
2021-12-15 01:03:54,112 [WARNING] Using custom data configuration default-5674deeddcf52035
2021-12-15 01:03:54,275 [WARNING] Using custom data configuration default-7111aaf4316c36e1
2021-12-15 01:03:54,427 [WARNING] Using custom data configuration default-f2826281445e588b
2021-12-15 01:03:54,561 [INFO] Done loading all the datasets
2021-12-15 01:03:54,562 [INFO] ============================================================
2021-12-15 01:03:54,562 [INFO] Starting training with the following hyperparameters: {'batch_size': 32, 'bidirectional': True, 'dropout': 0.2, 'early_stopping_threshold': 10, 'hidden_dim': 256, 'lr': 0.001, 'max_seq_length': 512, 'model_type': 'lstm', 'num_epochs': 100, 'num_layers': 1}
2021-12-15 01:03:54,738 [INFO] Model created...
2021-12-15 01:03:54,739 [INFO] ===========================================================
2021-12-15 01:03:54,741 [INFO] Training model HighwayAugmenter(
  (masking_model): RNN(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (rnn): LSTM(768, 256, batch_first=True, bidirectional=True)
    (dropout): Dropout(p=0.2, inplace=False)
    (dense): Linear(in_features=512, out_features=2, bias=True)
  )
  (unmasking_model): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): RNN(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (rnn): LSTM(770, 256, batch_first=True, bidirectional=True)
    (dropout): Dropout(p=0.2, inplace=False)
    (dense): Linear(in_features=512, out_features=5, bias=True)
  )
)
2021-12-15 01:03:54,741 [INFO] ***********************************************************
2021-12-15 01:03:54,742 [INFO] Training epoch 0
2021-12-15 01:04:18,924 [INFO] | epoch   0 |    10/   11 batches | training accuracy    0.601
2021-12-15 01:04:21,496 [INFO] ***********************************************************
2021-12-15 01:04:21,497 [INFO] | end of epoch   0 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:04:21,497 [INFO] ***********************************************************
2021-12-15 01:04:21,561 [INFO] ***********************************************************
2021-12-15 01:04:21,561 [INFO] Training epoch 1
2021-12-15 01:04:37,152 [INFO] | epoch   1 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:04:39,745 [INFO] ***********************************************************
2021-12-15 01:04:39,746 [INFO] | end of epoch   1 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:04:39,746 [INFO] ***********************************************************
2021-12-15 01:04:39,748 [INFO] ***********************************************************
2021-12-15 01:04:39,748 [INFO] Training epoch 2
2021-12-15 01:04:55,580 [INFO] | epoch   2 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:04:58,272 [INFO] ***********************************************************
2021-12-15 01:04:58,273 [INFO] | end of epoch   2 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:04:58,274 [INFO] ***********************************************************
2021-12-15 01:04:58,275 [INFO] ***********************************************************
2021-12-15 01:04:58,275 [INFO] Training epoch 3
2021-12-15 01:05:14,132 [INFO] | epoch   3 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:05:16,851 [INFO] ***********************************************************
2021-12-15 01:05:16,852 [INFO] | end of epoch   3 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:05:16,852 [INFO] ***********************************************************
2021-12-15 01:05:16,854 [INFO] ***********************************************************
2021-12-15 01:05:16,854 [INFO] Training epoch 4
2021-12-15 01:05:32,415 [INFO] | epoch   4 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:05:35,036 [INFO] ***********************************************************
2021-12-15 01:05:35,037 [INFO] | end of epoch   4 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:05:35,037 [INFO] ***********************************************************
2021-12-15 01:05:35,040 [INFO] ***********************************************************
2021-12-15 01:05:35,040 [INFO] Training epoch 5
2021-12-15 01:05:50,754 [INFO] | epoch   5 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:05:53,354 [INFO] ***********************************************************
2021-12-15 01:05:53,354 [INFO] | end of epoch   5 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:05:53,355 [INFO] ***********************************************************
2021-12-15 01:05:53,356 [INFO] ***********************************************************
2021-12-15 01:05:53,356 [INFO] Training epoch 6
2021-12-15 01:06:08,996 [INFO] | epoch   6 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:06:11,617 [INFO] ***********************************************************
2021-12-15 01:06:11,618 [INFO] | end of epoch   6 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:06:11,618 [INFO] ***********************************************************
2021-12-15 01:06:11,620 [INFO] ***********************************************************
2021-12-15 01:06:11,620 [INFO] Training epoch 7
2021-12-15 01:06:27,458 [INFO] | epoch   7 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:06:30,053 [INFO] ***********************************************************
2021-12-15 01:06:30,054 [INFO] | end of epoch   7 valid accuracy    0.600 and f1 score    0.151
2021-12-15 01:06:30,054 [INFO] ***********************************************************
2021-12-15 01:06:30,056 [INFO] ***********************************************************
2021-12-15 01:06:30,056 [INFO] Training epoch 8
2021-12-15 01:06:45,742 [INFO] | epoch   8 |    10/   11 batches | training accuracy    0.675
2021-12-15 01:06:48,390 [INFO] ***********************************************************
2021-12-15 01:06:48,391 [INFO] | end of epoch   8 valid accuracy    0.600 and f1 score    0.206
2021-12-15 01:06:48,391 [INFO] ***********************************************************
2021-12-15 01:06:48,446 [INFO] ***********************************************************
2021-12-15 01:06:48,446 [INFO] Training epoch 9
2021-12-15 01:07:04,142 [INFO] | epoch   9 |    10/   11 batches | training accuracy    0.755
2021-12-15 01:07:06,754 [INFO] ***********************************************************
2021-12-15 01:07:06,754 [INFO] | end of epoch   9 valid accuracy    0.573 and f1 score    0.237
2021-12-15 01:07:06,755 [INFO] ***********************************************************
2021-12-15 01:07:06,814 [INFO] ***********************************************************
2021-12-15 01:07:06,815 [INFO] Training epoch 10
2021-12-15 01:07:22,711 [INFO] | epoch  10 |    10/   11 batches | training accuracy    0.769
2021-12-15 01:07:25,340 [INFO] ***********************************************************
2021-12-15 01:07:25,341 [INFO] | end of epoch  10 valid accuracy    0.600 and f1 score    0.215
2021-12-15 01:07:25,341 [INFO] ***********************************************************
2021-12-15 01:07:25,342 [INFO] ***********************************************************
2021-12-15 01:07:25,342 [INFO] Training epoch 11
2021-12-15 01:07:41,116 [INFO] | epoch  11 |    10/   11 batches | training accuracy    0.778
2021-12-15 01:07:43,724 [INFO] ***********************************************************
2021-12-15 01:07:43,725 [INFO] | end of epoch  11 valid accuracy    0.480 and f1 score    0.204
2021-12-15 01:07:43,726 [INFO] ***********************************************************
2021-12-15 01:07:43,728 [INFO] ***********************************************************
2021-12-15 01:07:43,729 [INFO] Training epoch 12
2021-12-15 01:07:59,402 [INFO] | epoch  12 |    10/   11 batches | training accuracy    0.789
2021-12-15 01:08:02,014 [INFO] ***********************************************************
2021-12-15 01:08:02,015 [INFO] | end of epoch  12 valid accuracy    0.587 and f1 score    0.243
2021-12-15 01:08:02,015 [INFO] ***********************************************************
2021-12-15 01:08:02,089 [INFO] ***********************************************************
2021-12-15 01:08:02,090 [INFO] Training epoch 13
2021-12-15 01:08:17,707 [INFO] | epoch  13 |    10/   11 batches | training accuracy    0.798
2021-12-15 01:08:20,330 [INFO] ***********************************************************
2021-12-15 01:08:20,330 [INFO] | end of epoch  13 valid accuracy    0.627 and f1 score    0.235
2021-12-15 01:08:20,331 [INFO] ***********************************************************
2021-12-15 01:08:20,332 [INFO] ***********************************************************
2021-12-15 01:08:20,333 [INFO] Training epoch 14
2021-12-15 01:08:36,148 [INFO] | epoch  14 |    10/   11 batches | training accuracy    0.809
2021-12-15 01:08:38,745 [INFO] ***********************************************************
2021-12-15 01:08:38,745 [INFO] | end of epoch  14 valid accuracy    0.573 and f1 score    0.246
2021-12-15 01:08:38,746 [INFO] ***********************************************************
2021-12-15 01:08:38,803 [INFO] ***********************************************************
2021-12-15 01:08:38,804 [INFO] Training epoch 15
2021-12-15 01:08:54,307 [INFO] | epoch  15 |    10/   11 batches | training accuracy    0.835
2021-12-15 01:08:56,872 [INFO] ***********************************************************
2021-12-15 01:08:56,872 [INFO] | end of epoch  15 valid accuracy    0.573 and f1 score    0.176
2021-12-15 01:08:56,873 [INFO] ***********************************************************
2021-12-15 01:08:56,874 [INFO] ***********************************************************
2021-12-15 01:08:56,875 [INFO] Training epoch 16
2021-12-15 01:09:12,614 [INFO] | epoch  16 |    10/   11 batches | training accuracy    0.875
2021-12-15 01:09:15,236 [INFO] ***********************************************************
2021-12-15 01:09:15,236 [INFO] | end of epoch  16 valid accuracy    0.600 and f1 score    0.248
2021-12-15 01:09:15,237 [INFO] ***********************************************************
2021-12-15 01:09:15,294 [INFO] ***********************************************************
2021-12-15 01:09:15,294 [INFO] Training epoch 17
2021-12-15 01:09:30,899 [INFO] | epoch  17 |    10/   11 batches | training accuracy    0.877
2021-12-15 01:09:33,550 [INFO] ***********************************************************
2021-12-15 01:09:33,551 [INFO] | end of epoch  17 valid accuracy    0.560 and f1 score    0.271
2021-12-15 01:09:33,551 [INFO] ***********************************************************
2021-12-15 01:09:33,609 [INFO] ***********************************************************
2021-12-15 01:09:33,609 [INFO] Training epoch 18
2021-12-15 01:09:49,433 [INFO] | epoch  18 |    10/   11 batches | training accuracy    0.917
2021-12-15 01:09:52,077 [INFO] ***********************************************************
2021-12-15 01:09:52,077 [INFO] | end of epoch  18 valid accuracy    0.547 and f1 score    0.232
2021-12-15 01:09:52,077 [INFO] ***********************************************************
2021-12-15 01:09:52,078 [INFO] ***********************************************************
2021-12-15 01:09:52,078 [INFO] Training epoch 19
2021-12-15 01:10:07,839 [INFO] | epoch  19 |    10/   11 batches | training accuracy    0.915
2021-12-15 01:10:10,531 [INFO] ***********************************************************
2021-12-15 01:10:10,532 [INFO] | end of epoch  19 valid accuracy    0.533 and f1 score    0.202
2021-12-15 01:10:10,532 [INFO] ***********************************************************
2021-12-15 01:10:10,533 [INFO] ***********************************************************
2021-12-15 01:10:10,533 [INFO] Training epoch 20
2021-12-15 01:10:26,179 [INFO] | epoch  20 |    10/   11 batches | training accuracy    0.932
2021-12-15 01:10:28,761 [INFO] ***********************************************************
2021-12-15 01:10:28,761 [INFO] | end of epoch  20 valid accuracy    0.547 and f1 score    0.215
2021-12-15 01:10:28,761 [INFO] ***********************************************************
2021-12-15 01:10:28,762 [INFO] ***********************************************************
2021-12-15 01:10:28,763 [INFO] Training epoch 21
2021-12-15 01:10:44,440 [INFO] | epoch  21 |    10/   11 batches | training accuracy    0.940
2021-12-15 01:10:47,059 [INFO] ***********************************************************
2021-12-15 01:10:47,060 [INFO] | end of epoch  21 valid accuracy    0.453 and f1 score    0.211
2021-12-15 01:10:47,060 [INFO] ***********************************************************
2021-12-15 01:10:47,061 [INFO] ***********************************************************
2021-12-15 01:10:47,062 [INFO] Training epoch 22
2021-12-15 01:11:02,746 [INFO] | epoch  22 |    10/   11 batches | training accuracy    0.943
2021-12-15 01:11:05,412 [INFO] ***********************************************************
2021-12-15 01:11:05,413 [INFO] | end of epoch  22 valid accuracy    0.440 and f1 score    0.180
2021-12-15 01:11:05,413 [INFO] ***********************************************************
2021-12-15 01:11:05,415 [INFO] ***********************************************************
2021-12-15 01:11:05,415 [INFO] Training epoch 23
2021-12-15 01:11:20,649 [INFO] | epoch  23 |    10/   11 batches | training accuracy    0.946
2021-12-15 01:11:22,371 [INFO] ***********************************************************
2021-12-15 01:11:22,371 [INFO] | end of epoch  23 valid accuracy    0.467 and f1 score    0.189
2021-12-15 01:11:22,372 [INFO] ***********************************************************
2021-12-15 01:11:22,373 [INFO] ***********************************************************
2021-12-15 01:11:22,373 [INFO] Training epoch 24
2021-12-15 01:11:32,634 [INFO] | epoch  24 |    10/   11 batches | training accuracy    0.949
2021-12-15 01:11:34,303 [INFO] ***********************************************************
2021-12-15 01:11:34,303 [INFO] | end of epoch  24 valid accuracy    0.507 and f1 score    0.210
2021-12-15 01:11:34,303 [INFO] ***********************************************************
2021-12-15 01:11:34,304 [INFO] ***********************************************************
2021-12-15 01:11:34,304 [INFO] Training epoch 25
2021-12-15 01:11:44,343 [INFO] | epoch  25 |    10/   11 batches | training accuracy    0.966
2021-12-15 01:11:45,992 [INFO] ***********************************************************
2021-12-15 01:11:45,993 [INFO] | end of epoch  25 valid accuracy    0.507 and f1 score    0.232
2021-12-15 01:11:45,993 [INFO] ***********************************************************
2021-12-15 01:11:45,994 [INFO] ***********************************************************
2021-12-15 01:11:45,994 [INFO] Training epoch 26
2021-12-15 01:11:56,077 [INFO] | epoch  26 |    10/   11 batches | training accuracy    0.969
2021-12-15 01:11:57,742 [INFO] ***********************************************************
2021-12-15 01:11:57,743 [INFO] | end of epoch  26 valid accuracy    0.467 and f1 score    0.223
2021-12-15 01:11:57,743 [INFO] ***********************************************************
2021-12-15 01:11:57,744 [INFO] ***********************************************************
2021-12-15 01:11:57,744 [INFO] Training epoch 27
2021-12-15 01:12:07,963 [INFO] | epoch  27 |    10/   11 batches | training accuracy    0.972
2021-12-15 01:12:09,659 [INFO] ***********************************************************
2021-12-15 01:12:09,659 [INFO] | end of epoch  27 valid accuracy    0.507 and f1 score    0.257
2021-12-15 01:12:09,659 [INFO] ***********************************************************
2021-12-15 01:12:09,660 [WARNING] The validation f1 score has not improved for 10 epochs. Stopping training early.
2021-12-15 01:12:09,660 [INFO] ============================================================
2021-12-15 01:12:09,660 [INFO] Starting training with the following hyperparameters: {'batch_size': 32, 'bidirectional': False, 'dropout': 0.2, 'early_stopping_threshold': 10, 'hidden_dim': 256, 'lr': 0.001, 'max_seq_length': 512, 'model_type': 'lstm', 'num_epochs': 100, 'num_layers': 1}
2021-12-15 01:12:09,688 [INFO] Model created...
2021-12-15 01:12:09,689 [INFO] ===========================================================
2021-12-15 01:12:09,690 [INFO] Training model HighwayAugmenter(
  (masking_model): RNN(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (rnn): LSTM(768, 256, batch_first=True)
    (dropout): Dropout(p=0.2, inplace=False)
    (dense): Linear(in_features=256, out_features=2, bias=True)
  )
  (unmasking_model): DistilBertModel(
    (embeddings): Embeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (transformer): Transformer(
      (layer): ModuleList(
        (0): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (1): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (2): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (3): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (4): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
        (5): TransformerBlock(
          (attention): MultiHeadSelfAttention(
            (dropout): Dropout(p=0.1, inplace=False)
            (q_lin): Linear(in_features=768, out_features=768, bias=True)
            (k_lin): Linear(in_features=768, out_features=768, bias=True)
            (v_lin): Linear(in_features=768, out_features=768, bias=True)
            (out_lin): Linear(in_features=768, out_features=768, bias=True)
          )
          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (ffn): FFN(
            (dropout): Dropout(p=0.1, inplace=False)
            (lin1): Linear(in_features=768, out_features=3072, bias=True)
            (lin2): Linear(in_features=3072, out_features=768, bias=True)
          )
          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
  (classifier): RNN(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (rnn): LSTM(770, 256, batch_first=True)
    (dropout): Dropout(p=0.2, inplace=False)
    (dense): Linear(in_features=256, out_features=5, bias=True)
  )
)
2021-12-15 01:12:09,691 [INFO] ***********************************************************
2021-12-15 01:12:09,691 [INFO] Training epoch 0
2021-12-15 01:12:18,549 [INFO] | epoch   0 |    10/   11 batches | training accuracy    0.584
2021-12-15 01:12:20,132 [INFO] ***********************************************************
2021-12-15 01:12:20,133 [INFO] | end of epoch   0 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:12:20,133 [INFO] ***********************************************************
2021-12-15 01:12:20,166 [INFO] ***********************************************************
2021-12-15 01:12:20,166 [INFO] Training epoch 1
2021-12-15 01:12:29,020 [INFO] | epoch   1 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:12:30,594 [INFO] ***********************************************************
2021-12-15 01:12:30,594 [INFO] | end of epoch   1 valid accuracy    0.613 and f1 score    0.215
2021-12-15 01:12:30,594 [INFO] ***********************************************************
2021-12-15 01:12:30,653 [INFO] ***********************************************************
2021-12-15 01:12:30,653 [INFO] Training epoch 2
2021-12-15 01:12:39,674 [INFO] | epoch   2 |    10/   11 batches | training accuracy    0.638
2021-12-15 01:12:41,240 [INFO] ***********************************************************
2021-12-15 01:12:41,241 [INFO] | end of epoch   2 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:12:41,241 [INFO] ***********************************************************
2021-12-15 01:12:41,242 [INFO] ***********************************************************
2021-12-15 01:12:41,242 [INFO] Training epoch 3
2021-12-15 01:12:50,105 [INFO] | epoch   3 |    10/   11 batches | training accuracy    0.664
2021-12-15 01:12:51,678 [INFO] ***********************************************************
2021-12-15 01:12:51,678 [INFO] | end of epoch   3 valid accuracy    0.453 and f1 score    0.188
2021-12-15 01:12:51,678 [INFO] ***********************************************************
2021-12-15 01:12:51,680 [INFO] ***********************************************************
2021-12-15 01:12:51,680 [INFO] Training epoch 4
2021-12-15 01:13:00,523 [INFO] | epoch   4 |    10/   11 batches | training accuracy    0.672
2021-12-15 01:13:02,078 [INFO] ***********************************************************
2021-12-15 01:13:02,079 [INFO] | end of epoch   4 valid accuracy    0.547 and f1 score    0.178
2021-12-15 01:13:02,079 [INFO] ***********************************************************
2021-12-15 01:13:02,080 [INFO] ***********************************************************
2021-12-15 01:13:02,080 [INFO] Training epoch 5
2021-12-15 01:13:10,971 [INFO] | epoch   5 |    10/   11 batches | training accuracy    0.707
2021-12-15 01:13:12,540 [INFO] ***********************************************************
2021-12-15 01:13:12,540 [INFO] | end of epoch   5 valid accuracy    0.493 and f1 score    0.195
2021-12-15 01:13:12,541 [INFO] ***********************************************************
2021-12-15 01:13:12,542 [INFO] ***********************************************************
2021-12-15 01:13:12,542 [INFO] Training epoch 6
2021-12-15 01:13:21,280 [INFO] | epoch   6 |    10/   11 batches | training accuracy    0.752
2021-12-15 01:13:22,838 [INFO] ***********************************************************
2021-12-15 01:13:22,838 [INFO] | end of epoch   6 valid accuracy    0.547 and f1 score    0.164
2021-12-15 01:13:22,838 [INFO] ***********************************************************
2021-12-15 01:13:22,839 [INFO] ***********************************************************
2021-12-15 01:13:22,840 [INFO] Training epoch 7
2021-12-15 01:13:31,701 [INFO] | epoch   7 |    10/   11 batches | training accuracy    0.786
2021-12-15 01:13:33,271 [INFO] ***********************************************************
2021-12-15 01:13:33,271 [INFO] | end of epoch   7 valid accuracy    0.480 and f1 score    0.180
2021-12-15 01:13:33,271 [INFO] ***********************************************************
2021-12-15 01:13:33,272 [INFO] ***********************************************************
2021-12-15 01:13:33,272 [INFO] Training epoch 8
2021-12-15 01:13:42,075 [INFO] | epoch   8 |    10/   11 batches | training accuracy    0.818
2021-12-15 01:13:43,570 [INFO] ***********************************************************
2021-12-15 01:13:43,570 [INFO] | end of epoch   8 valid accuracy    0.520 and f1 score    0.173
2021-12-15 01:13:43,571 [INFO] ***********************************************************
2021-12-15 01:13:43,571 [INFO] ***********************************************************
2021-12-15 01:13:43,572 [INFO] Training epoch 9
2021-12-15 01:13:52,304 [INFO] | epoch   9 |    10/   11 batches | training accuracy    0.835
2021-12-15 01:13:53,849 [INFO] ***********************************************************
2021-12-15 01:13:53,849 [INFO] | end of epoch   9 valid accuracy    0.440 and f1 score    0.145
2021-12-15 01:13:53,849 [INFO] ***********************************************************
2021-12-15 01:13:53,850 [INFO] ***********************************************************
2021-12-15 01:13:53,850 [INFO] Training epoch 10
2021-12-15 01:14:02,806 [INFO] | epoch  10 |    10/   11 batches | training accuracy    0.855
2021-12-15 01:14:04,382 [INFO] ***********************************************************
2021-12-15 01:14:04,382 [INFO] | end of epoch  10 valid accuracy    0.413 and f1 score    0.169
2021-12-15 01:14:04,382 [INFO] ***********************************************************
2021-12-15 01:14:04,383 [INFO] ***********************************************************
2021-12-15 01:14:04,383 [INFO] Training epoch 11
2021-12-15 01:14:13,142 [INFO] | epoch  11 |    10/   11 batches | training accuracy    0.866
2021-12-15 01:14:14,710 [INFO] ***********************************************************
2021-12-15 01:14:14,710 [INFO] | end of epoch  11 valid accuracy    0.427 and f1 score    0.189
2021-12-15 01:14:14,710 [INFO] ***********************************************************
2021-12-15 01:14:14,711 [WARNING] The validation f1 score has not improved for 10 epochs. Stopping training early.
2021-12-15 01:14:14,712 [INFO] ============================================================
2021-12-15 01:14:14,712 [INFO] Finished training model. Best hyperparameters: {'batch_size': 32, 'bidirectional': True, 'dropout': 0.2, 'early_stopping_threshold': 10, 'hidden_dim': 256, 'lr': 0.001, 'max_seq_length': 512, 'model_type': 'lstm', 'num_epochs': 100, 'num_layers': 1}
2021-12-15 01:14:14,712 [INFO] Here is the validation results:
2021-12-15 01:14:16,316 [INFO] ***********************************************************
2021-12-15 01:14:16,316 [INFO] |valid accuracy    0.560 and f1 score    0.271
2021-12-15 01:14:16,320 [INFO]               precision    recall  f1-score   support

           0       0.74      0.74      0.74        47
           1       0.29      0.60      0.39        10
           2       0.00      0.00      0.00         9
           3       0.00      0.00      0.00         2
           4       0.50      0.14      0.22         7

    accuracy                           0.56        75
   macro avg       0.31      0.30      0.27        75
weighted avg       0.55      0.56      0.54        75

2021-12-15 01:14:16,321 [INFO] [[35  9  1  2  0]
 [ 3  6  0  1  0]
 [ 5  3  0  0  1]
 [ 1  0  1  0  0]
 [ 3  3  0  0  1]]
2021-12-15 01:14:16,321 [INFO] ***********************************************************
2021-12-15 01:14:16,323 [INFO] Here is the test results (DO NOT USE THESE RESULTS FOR CHOOSING THE BEST AUGMENTATION):
2021-12-15 01:14:17,994 [INFO] ***********************************************************
2021-12-15 01:14:17,994 [INFO] |valid accuracy    0.487 and f1 score    0.212
2021-12-15 01:14:18,000 [INFO]               precision    recall  f1-score   support

           0       0.60      0.73      0.66        45
           1       0.17      0.25      0.20        12
           2       0.00      0.00      0.00         7
           3       0.00      0.00      0.00         3
           4       1.00      0.11      0.20         9

    accuracy                           0.49        76
   macro avg       0.35      0.22      0.21        76
weighted avg       0.50      0.49      0.45        76

2021-12-15 01:14:18,001 [INFO] [[33 12  0  0  0]
 [ 8  3  0  1  0]
 [ 6  0  0  1  0]
 [ 1  2  0  0  0]
 [ 7  1  0  0  1]]
2021-12-15 01:14:18,002 [INFO] ***********************************************************
2021-12-15 01:14:19,255 [INFO] Model saved to ==> /content/models/smokers/model_distilbert_base_uncased.pt
2021-12-15 01:14:19,255 [INFO] ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
2021-12-15 01:14:19,256 [INFO] Starting training using the HuggingFace model microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext
2021-12-15 01:14:35,217 [WARNING] Using custom data configuration default-5674deeddcf52035
2021-12-15 01:14:35,218 [WARNING] Reusing dataset json (/root/.cache/huggingface/datasets/json/default-5674deeddcf52035/0.0.0/7334ed11270d48b1ef46b6251d4d612598d73f1cd0c2510f656d05a4ff0d849a)
2021-12-15 01:14:35,228 [WARNING] Using custom data configuration default-7111aaf4316c36e1
2021-12-15 01:14:35,228 [WARNING] Reusing dataset json (/root/.cache/huggingface/datasets/json/default-7111aaf4316c36e1/0.0.0/7334ed11270d48b1ef46b6251d4d612598d73f1cd0c2510f656d05a4ff0d849a)
2021-12-15 01:14:35,237 [WARNING] Using custom data configuration default-f2826281445e588b
2021-12-15 01:14:35,238 [WARNING] Reusing dataset json (/root/.cache/huggingface/datasets/json/default-f2826281445e588b/0.0.0/7334ed11270d48b1ef46b6251d4d612598d73f1cd0c2510f656d05a4ff0d849a)
2021-12-15 01:14:35,239 [INFO] Done loading all the datasets
2021-12-15 01:14:35,248 [INFO] ============================================================
2021-12-15 01:14:35,248 [INFO] Starting training with the following hyperparameters: {'batch_size': 32, 'bidirectional': True, 'dropout': 0.2, 'early_stopping_threshold': 10, 'hidden_dim': 256, 'lr': 0.001, 'max_seq_length': 512, 'model_type': 'lstm', 'num_epochs': 100, 'num_layers': 1}
2021-12-15 01:14:35,314 [INFO] Model created...
2021-12-15 01:14:35,329 [INFO] ===========================================================
2021-12-15 01:14:35,331 [INFO] Training model HighwayAugmenter(
  (masking_model): RNN(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (rnn): LSTM(768, 256, batch_first=True, bidirectional=True)
    (dropout): Dropout(p=0.2, inplace=False)
    (dense): Linear(in_features=512, out_features=2, bias=True)
  )
  (unmasking_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): RNN(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (rnn): LSTM(770, 256, batch_first=True, bidirectional=True)
    (dropout): Dropout(p=0.2, inplace=False)
    (dense): Linear(in_features=512, out_features=5, bias=True)
  )
)
2021-12-15 01:14:35,332 [INFO] ***********************************************************
2021-12-15 01:14:35,332 [INFO] Training epoch 0
2021-12-15 01:14:51,722 [INFO] | epoch   0 |    10/   11 batches | training accuracy    0.604
2021-12-15 01:14:54,687 [INFO] ***********************************************************
2021-12-15 01:14:54,688 [INFO] | end of epoch   0 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:14:54,688 [INFO] ***********************************************************
2021-12-15 01:14:54,756 [INFO] ***********************************************************
2021-12-15 01:14:54,756 [INFO] Training epoch 1
2021-12-15 01:15:10,957 [INFO] | epoch   1 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:15:13,863 [INFO] ***********************************************************
2021-12-15 01:15:13,863 [INFO] | end of epoch   1 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:15:13,863 [INFO] ***********************************************************
2021-12-15 01:15:13,864 [INFO] ***********************************************************
2021-12-15 01:15:13,865 [INFO] Training epoch 2
2021-12-15 01:15:29,888 [INFO] | epoch   2 |    10/   11 batches | training accuracy    0.632
2021-12-15 01:15:32,821 [INFO] ***********************************************************
2021-12-15 01:15:32,822 [INFO] | end of epoch   2 valid accuracy    0.680 and f1 score    0.273
2021-12-15 01:15:32,822 [INFO] ***********************************************************
2021-12-15 01:15:32,902 [INFO] ***********************************************************
2021-12-15 01:15:32,903 [INFO] Training epoch 3
2021-12-15 01:15:49,014 [INFO] | epoch   3 |    10/   11 batches | training accuracy    0.638
2021-12-15 01:15:51,885 [INFO] ***********************************************************
2021-12-15 01:15:51,886 [INFO] | end of epoch   3 valid accuracy    0.667 and f1 score    0.245
2021-12-15 01:15:51,886 [INFO] ***********************************************************
2021-12-15 01:15:51,888 [INFO] ***********************************************************
2021-12-15 01:15:51,888 [INFO] Training epoch 4
2021-12-15 01:16:08,054 [INFO] | epoch   4 |    10/   11 batches | training accuracy    0.627
2021-12-15 01:16:10,924 [INFO] ***********************************************************
2021-12-15 01:16:10,924 [INFO] | end of epoch   4 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:16:10,924 [INFO] ***********************************************************
2021-12-15 01:16:10,926 [INFO] ***********************************************************
2021-12-15 01:16:10,926 [INFO] Training epoch 5
2021-12-15 01:16:26,934 [INFO] | epoch   5 |    10/   11 batches | training accuracy    0.650
2021-12-15 01:16:29,838 [INFO] ***********************************************************
2021-12-15 01:16:29,838 [INFO] | end of epoch   5 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:16:29,838 [INFO] ***********************************************************
2021-12-15 01:16:29,839 [INFO] ***********************************************************
2021-12-15 01:16:29,840 [INFO] Training epoch 6
2021-12-15 01:16:46,146 [INFO] | epoch   6 |    10/   11 batches | training accuracy    0.687
2021-12-15 01:16:49,041 [INFO] ***********************************************************
2021-12-15 01:16:49,042 [INFO] | end of epoch   6 valid accuracy    0.653 and f1 score    0.223
2021-12-15 01:16:49,042 [INFO] ***********************************************************
2021-12-15 01:16:49,043 [INFO] ***********************************************************
2021-12-15 01:16:49,043 [INFO] Training epoch 7
2021-12-15 01:17:05,290 [INFO] | epoch   7 |    10/   11 batches | training accuracy    0.758
2021-12-15 01:17:08,181 [INFO] ***********************************************************
2021-12-15 01:17:08,181 [INFO] | end of epoch   7 valid accuracy    0.573 and f1 score    0.210
2021-12-15 01:17:08,181 [INFO] ***********************************************************
2021-12-15 01:17:08,182 [INFO] ***********************************************************
2021-12-15 01:17:08,182 [INFO] Training epoch 8
2021-12-15 01:17:24,111 [INFO] | epoch   8 |    10/   11 batches | training accuracy    0.798
2021-12-15 01:17:27,013 [INFO] ***********************************************************
2021-12-15 01:17:27,013 [INFO] | end of epoch   8 valid accuracy    0.653 and f1 score    0.254
2021-12-15 01:17:27,013 [INFO] ***********************************************************
2021-12-15 01:17:27,014 [INFO] ***********************************************************
2021-12-15 01:17:27,014 [INFO] Training epoch 9
2021-12-15 01:17:43,261 [INFO] | epoch   9 |    10/   11 batches | training accuracy    0.783
2021-12-15 01:17:46,191 [INFO] ***********************************************************
2021-12-15 01:17:46,191 [INFO] | end of epoch   9 valid accuracy    0.560 and f1 score    0.194
2021-12-15 01:17:46,191 [INFO] ***********************************************************
2021-12-15 01:17:46,192 [INFO] ***********************************************************
2021-12-15 01:17:46,192 [INFO] Training epoch 10
2021-12-15 01:18:02,469 [INFO] | epoch  10 |    10/   11 batches | training accuracy    0.783
2021-12-15 01:18:05,347 [INFO] ***********************************************************
2021-12-15 01:18:05,347 [INFO] | end of epoch  10 valid accuracy    0.507 and f1 score    0.219
2021-12-15 01:18:05,347 [INFO] ***********************************************************
2021-12-15 01:18:05,348 [INFO] ***********************************************************
2021-12-15 01:18:05,349 [INFO] Training epoch 11
2021-12-15 01:18:21,449 [INFO] | epoch  11 |    10/   11 batches | training accuracy    0.812
2021-12-15 01:18:24,352 [INFO] ***********************************************************
2021-12-15 01:18:24,353 [INFO] | end of epoch  11 valid accuracy    0.520 and f1 score    0.202
2021-12-15 01:18:24,353 [INFO] ***********************************************************
2021-12-15 01:18:24,354 [INFO] ***********************************************************
2021-12-15 01:18:24,354 [INFO] Training epoch 12
2021-12-15 01:18:40,599 [INFO] | epoch  12 |    10/   11 batches | training accuracy    0.795
2021-12-15 01:18:43,473 [INFO] ***********************************************************
2021-12-15 01:18:43,473 [INFO] | end of epoch  12 valid accuracy    0.520 and f1 score    0.199
2021-12-15 01:18:43,473 [INFO] ***********************************************************
2021-12-15 01:18:43,474 [WARNING] The validation f1 score has not improved for 10 epochs. Stopping training early.
2021-12-15 01:18:43,475 [INFO] ============================================================
2021-12-15 01:18:43,475 [INFO] Starting training with the following hyperparameters: {'batch_size': 32, 'bidirectional': False, 'dropout': 0.2, 'early_stopping_threshold': 10, 'hidden_dim': 256, 'lr': 0.001, 'max_seq_length': 512, 'model_type': 'lstm', 'num_epochs': 100, 'num_layers': 1}
2021-12-15 01:18:43,499 [INFO] Model created...
2021-12-15 01:18:43,499 [INFO] ===========================================================
2021-12-15 01:18:43,502 [INFO] Training model HighwayAugmenter(
  (masking_model): RNN(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (rnn): LSTM(768, 256, batch_first=True)
    (dropout): Dropout(p=0.2, inplace=False)
    (dense): Linear(in_features=256, out_features=2, bias=True)
  )
  (unmasking_model): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (classifier): RNN(
    (embeddings): Embedding(30522, 768, padding_idx=0)
    (rnn): LSTM(770, 256, batch_first=True)
    (dropout): Dropout(p=0.2, inplace=False)
    (dense): Linear(in_features=256, out_features=5, bias=True)
  )
)
2021-12-15 01:18:43,502 [INFO] ***********************************************************
2021-12-15 01:18:43,502 [INFO] Training epoch 0
2021-12-15 01:18:58,449 [INFO] | epoch   0 |    10/   11 batches | training accuracy    0.584
2021-12-15 01:19:01,247 [INFO] ***********************************************************
2021-12-15 01:19:01,247 [INFO] | end of epoch   0 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:19:01,248 [INFO] ***********************************************************
2021-12-15 01:19:01,308 [INFO] ***********************************************************
2021-12-15 01:19:01,308 [INFO] Training epoch 1
2021-12-15 01:19:16,212 [INFO] | epoch   1 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:19:19,002 [INFO] ***********************************************************
2021-12-15 01:19:19,003 [INFO] | end of epoch   1 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:19:19,003 [INFO] ***********************************************************
2021-12-15 01:19:19,004 [INFO] ***********************************************************
2021-12-15 01:19:19,004 [INFO] Training epoch 2
2021-12-15 01:19:34,034 [INFO] | epoch   2 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:19:36,838 [INFO] ***********************************************************
2021-12-15 01:19:36,838 [INFO] | end of epoch   2 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:19:36,838 [INFO] ***********************************************************
2021-12-15 01:19:36,839 [INFO] ***********************************************************
2021-12-15 01:19:36,839 [INFO] Training epoch 3
2021-12-15 01:19:51,741 [INFO] | epoch   3 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:19:54,728 [INFO] ***********************************************************
2021-12-15 01:19:54,728 [INFO] | end of epoch   3 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:19:54,728 [INFO] ***********************************************************
2021-12-15 01:19:54,729 [INFO] ***********************************************************
2021-12-15 01:19:54,729 [INFO] Training epoch 4
2021-12-15 01:20:09,656 [INFO] | epoch   4 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:20:12,451 [INFO] ***********************************************************
2021-12-15 01:20:12,452 [INFO] | end of epoch   4 valid accuracy    0.480 and f1 score    0.202
2021-12-15 01:20:12,452 [INFO] ***********************************************************
2021-12-15 01:20:12,519 [INFO] ***********************************************************
2021-12-15 01:20:12,519 [INFO] Training epoch 5
2021-12-15 01:20:27,492 [INFO] | epoch   5 |    10/   11 batches | training accuracy    0.621
2021-12-15 01:20:30,320 [INFO] ***********************************************************
2021-12-15 01:20:30,321 [INFO] | end of epoch   5 valid accuracy    0.627 and f1 score    0.154
2021-12-15 01:20:30,321 [INFO] ***********************************************************
2021-12-15 01:20:30,322 [INFO] ***********************************************************
2021-12-15 01:20:30,322 [INFO] Training epoch 6
2021-12-15 01:20:45,162 [INFO] | epoch   6 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:20:47,964 [INFO] ***********************************************************
2021-12-15 01:20:47,964 [INFO] | end of epoch   6 valid accuracy    0.480 and f1 score    0.205
2021-12-15 01:20:47,964 [INFO] ***********************************************************
2021-12-15 01:20:48,023 [INFO] ***********************************************************
2021-12-15 01:20:48,023 [INFO] Training epoch 7
2021-12-15 01:21:02,965 [INFO] | epoch   7 |    10/   11 batches | training accuracy    0.635
2021-12-15 01:21:05,767 [INFO] ***********************************************************
2021-12-15 01:21:05,767 [INFO] | end of epoch   7 valid accuracy    0.520 and f1 score    0.214
2021-12-15 01:21:05,767 [INFO] ***********************************************************
2021-12-15 01:21:05,821 [INFO] ***********************************************************
2021-12-15 01:21:05,822 [INFO] Training epoch 8
2021-12-15 01:21:20,777 [INFO] | epoch   8 |    10/   11 batches | training accuracy    0.632
2021-12-15 01:21:23,617 [INFO] ***********************************************************
2021-12-15 01:21:23,617 [INFO] | end of epoch   8 valid accuracy    0.480 and f1 score    0.201
2021-12-15 01:21:23,617 [INFO] ***********************************************************
2021-12-15 01:21:23,619 [INFO] ***********************************************************
2021-12-15 01:21:23,619 [INFO] Training epoch 9
2021-12-15 01:21:38,462 [INFO] | epoch   9 |    10/   11 batches | training accuracy    0.647
2021-12-15 01:21:41,262 [INFO] ***********************************************************
2021-12-15 01:21:41,262 [INFO] | end of epoch   9 valid accuracy    0.467 and f1 score    0.201
2021-12-15 01:21:41,262 [INFO] ***********************************************************
2021-12-15 01:21:41,263 [INFO] ***********************************************************
2021-12-15 01:21:41,264 [INFO] Training epoch 10
2021-12-15 01:21:56,323 [INFO] | epoch  10 |    10/   11 batches | training accuracy    0.718
2021-12-15 01:21:59,125 [INFO] ***********************************************************
2021-12-15 01:21:59,125 [INFO] | end of epoch  10 valid accuracy    0.587 and f1 score    0.230
2021-12-15 01:21:59,125 [INFO] ***********************************************************
2021-12-15 01:21:59,320 [INFO] ***********************************************************
2021-12-15 01:21:59,320 [INFO] Training epoch 11
2021-12-15 01:22:14,114 [INFO] | epoch  11 |    10/   11 batches | training accuracy    0.749
2021-12-15 01:22:16,886 [INFO] ***********************************************************
2021-12-15 01:22:16,886 [INFO] | end of epoch  11 valid accuracy    0.467 and f1 score    0.201
2021-12-15 01:22:16,887 [INFO] ***********************************************************
2021-12-15 01:22:16,888 [INFO] ***********************************************************
2021-12-15 01:22:16,888 [INFO] Training epoch 12
2021-12-15 01:22:32,004 [INFO] | epoch  12 |    10/   11 batches | training accuracy    0.766
2021-12-15 01:22:34,872 [INFO] ***********************************************************
2021-12-15 01:22:34,872 [INFO] | end of epoch  12 valid accuracy    0.467 and f1 score    0.187
2021-12-15 01:22:34,872 [INFO] ***********************************************************
2021-12-15 01:22:34,873 [INFO] ***********************************************************
2021-12-15 01:22:34,873 [INFO] Training epoch 13
2021-12-15 01:22:49,933 [INFO] | epoch  13 |    10/   11 batches | training accuracy    0.795
2021-12-15 01:22:52,713 [INFO] ***********************************************************
2021-12-15 01:22:52,713 [INFO] | end of epoch  13 valid accuracy    0.600 and f1 score    0.220
2021-12-15 01:22:52,714 [INFO] ***********************************************************
2021-12-15 01:22:52,715 [INFO] ***********************************************************
2021-12-15 01:22:52,715 [INFO] Training epoch 14
2021-12-15 01:23:07,567 [INFO] | epoch  14 |    10/   11 batches | training accuracy    0.789
2021-12-15 01:23:10,401 [INFO] ***********************************************************
2021-12-15 01:23:10,401 [INFO] | end of epoch  14 valid accuracy    0.547 and f1 score    0.190
2021-12-15 01:23:10,401 [INFO] ***********************************************************
2021-12-15 01:23:10,402 [INFO] ***********************************************************
2021-12-15 01:23:10,402 [INFO] Training epoch 15
2021-12-15 01:23:25,417 [INFO] | epoch  15 |    10/   11 batches | training accuracy    0.786
2021-12-15 01:23:28,224 [INFO] ***********************************************************
2021-12-15 01:23:28,224 [INFO] | end of epoch  15 valid accuracy    0.507 and f1 score    0.193
2021-12-15 01:23:28,224 [INFO] ***********************************************************
2021-12-15 01:23:28,225 [INFO] ***********************************************************
2021-12-15 01:23:28,225 [INFO] Training epoch 16
2021-12-15 01:23:43,071 [INFO] | epoch  16 |    10/   11 batches | training accuracy    0.809
2021-12-15 01:23:46,022 [INFO] ***********************************************************
2021-12-15 01:23:46,023 [INFO] | end of epoch  16 valid accuracy    0.533 and f1 score    0.214
2021-12-15 01:23:46,023 [INFO] ***********************************************************
2021-12-15 01:23:46,024 [INFO] ***********************************************************
2021-12-15 01:23:46,025 [INFO] Training epoch 17
2021-12-15 01:24:00,803 [INFO] | epoch  17 |    10/   11 batches | training accuracy    0.806
2021-12-15 01:24:03,603 [INFO] ***********************************************************
2021-12-15 01:24:03,603 [INFO] | end of epoch  17 valid accuracy    0.520 and f1 score    0.204
2021-12-15 01:24:03,604 [INFO] ***********************************************************
2021-12-15 01:24:03,605 [INFO] ***********************************************************
2021-12-15 01:24:03,605 [INFO] Training epoch 18
2021-12-15 01:24:18,597 [INFO] | epoch  18 |    10/   11 batches | training accuracy    0.815
2021-12-15 01:24:21,386 [INFO] ***********************************************************
2021-12-15 01:24:21,387 [INFO] | end of epoch  18 valid accuracy    0.533 and f1 score    0.210
2021-12-15 01:24:21,387 [INFO] ***********************************************************
2021-12-15 01:24:21,388 [INFO] ***********************************************************
2021-12-15 01:24:21,388 [INFO] Training epoch 19
2021-12-15 01:24:36,197 [INFO] | epoch  19 |    10/   11 batches | training accuracy    0.832
2021-12-15 01:24:38,964 [INFO] ***********************************************************
2021-12-15 01:24:38,964 [INFO] | end of epoch  19 valid accuracy    0.573 and f1 score    0.212
2021-12-15 01:24:38,964 [INFO] ***********************************************************
2021-12-15 01:24:38,965 [INFO] ***********************************************************
2021-12-15 01:24:38,965 [INFO] Training epoch 20
2021-12-15 01:24:53,910 [INFO] | epoch  20 |    10/   11 batches | training accuracy    0.860
2021-12-15 01:24:56,703 [INFO] ***********************************************************
2021-12-15 01:24:56,704 [INFO] | end of epoch  20 valid accuracy    0.480 and f1 score    0.198
2021-12-15 01:24:56,704 [INFO] ***********************************************************
2021-12-15 01:24:56,705 [WARNING] The validation f1 score has not improved for 10 epochs. Stopping training early.
2021-12-15 01:24:56,705 [INFO] ============================================================
2021-12-15 01:24:56,706 [INFO] Finished training model. Best hyperparameters: {'batch_size': 32, 'bidirectional': True, 'dropout': 0.2, 'early_stopping_threshold': 10, 'hidden_dim': 256, 'lr': 0.001, 'max_seq_length': 512, 'model_type': 'lstm', 'num_epochs': 100, 'num_layers': 1}
2021-12-15 01:24:56,706 [INFO] Here is the validation results:
2021-12-15 01:24:59,572 [INFO] ***********************************************************
2021-12-15 01:24:59,572 [INFO] |valid accuracy    0.680 and f1 score    0.273
2021-12-15 01:24:59,577 [INFO]               precision    recall  f1-score   support

           0       0.69      0.98      0.81        47
           1       0.62      0.50      0.56        10
           2       0.00      0.00      0.00         9
           3       0.00      0.00      0.00         2
           4       0.00      0.00      0.00         7

    accuracy                           0.68        75
   macro avg       0.26      0.30      0.27        75
weighted avg       0.51      0.68      0.58        75

2021-12-15 01:24:59,578 [INFO] [[46  1  0  0  0]
 [ 5  5  0  0  0]
 [ 8  1  0  0  0]
 [ 2  0  0  0  0]
 [ 6  1  0  0  0]]
2021-12-15 01:24:59,579 [INFO] ***********************************************************
2021-12-15 01:24:59,580 [INFO] Here is the test results (DO NOT USE THESE RESULTS FOR CHOOSING THE BEST AUGMENTATION):
2021-12-15 01:25:02,504 [INFO] ***********************************************************
2021-12-15 01:25:02,504 [INFO] |valid accuracy    0.592 and f1 score    0.209
2021-12-15 01:25:02,510 [INFO]               precision    recall  f1-score   support

           0       0.62      0.93      0.74        45
           1       0.38      0.25      0.30        12
           2       0.00      0.00      0.00         7
           3       0.00      0.00      0.00         3
           4       0.00      0.00      0.00         9

    accuracy                           0.59        76
   macro avg       0.20      0.24      0.21        76
weighted avg       0.42      0.59      0.49        76

2021-12-15 01:25:02,511 [INFO] [[42  3  0  0  0]
 [ 9  3  0  0  0]
 [ 6  1  0  0  0]
 [ 3  0  0  0  0]
 [ 8  1  0  0  0]]
2021-12-15 01:25:02,511 [INFO] ***********************************************************
2021-12-15 01:25:04,025 [INFO] Model saved to ==> /content/models/smokers/model_pubmed_bert_base_uncased.pt
