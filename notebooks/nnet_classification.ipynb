{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the legacy approach :'("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext.legacy\n",
    "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c_spino/anaconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "# Train, val and test are all\n",
    "# {\"text\": \"...\", \"label\": \"...\"}\n",
    "# Use the spacy tokenizer\n",
    "tokenizer=spacy.load('en_core_web_sm')\n",
    "spacy_tokenizer = lambda x: [tok.text for tok in tokenizer(x)]\n",
    "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.int8)\n",
    "text_field = Field(sequential=True, use_vocab=True, tokenize=\"spacy\", lower=True, include_lengths=True, batch_first=True, dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{None: None,\n",
       " 'text': <torchtext.legacy.data.field.Field at 0x7feb9e64b4d0>,\n",
       " 'label': <torchtext.legacy.data.field.Field at 0x7feb9f30df90>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_field = dict([(None, None), (\"text\", text_field), (\"label\", label_field)])\n",
    "data_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TabularDataset(\"/home/c_spino/comp_550/comp-550-project/data/rt-polaritydata/augmentation/validation.json\", format=\"json\", fields=data_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from src.utils.json_utils import read_json_lines\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "import torch\n",
    "from torch import nn\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c_spino/anaconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "class JsonDataset(Dataset):\n",
    "    def __init__(self, json_file_path, transform=None):\n",
    "        json_lines = read_json_lines(json_file_path)\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"text\": [json_dict[\"text\"] for json_dict in json_lines],\n",
    "                \"label\": [json_dict[\"label\"] for json_dict in json_lines]\n",
    "                }\n",
    "        )\n",
    "        self.dataset = df\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = {\n",
    "            \"text\": self.dataset.iloc[idx][\"text\"],\n",
    "            \"label\": self.dataset.iloc[idx][\"label\"]\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "def yield_tokens(dataset, tokenizer):\n",
    "    for sample in dataset:\n",
    "        yield tokenizer(sample[\"text\"])\n",
    "\n",
    "\n",
    "def build_vocab(dataset, tokenizer, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for sample in dataset:\n",
    "        counter.update(tokenizer(sample[\"text\"]))\n",
    "    counter = Counter({word: count for word, count in counter.items() if count >= min_freq})\n",
    "    return Vocab(counter, specials=['<unk>', '<pad>'])\n",
    "\n",
    "\n",
    "tokenizer = get_tokenizer('spacy', language='en')\n",
    "val_dataset = JsonDataset(\"/home/c_spino/comp_550/comp-550-project/data/rt-polaritydata/augmentation/validation.json\")\n",
    "vocab_ = build_vocab_from_iterator(yield_tokens(val_dataset, tokenizer), min_freq=1, specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab_.set_default_index(vocab_[\"<unk>\"])\n",
    "\n",
    "\n",
    "text_pipeline = lambda x: vocab_(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_batch, text_batch = [], []\n",
    "    # Need to pad the text\n",
    "    for sample in batch:\n",
    "        _text = sample[\"text\"]\n",
    "        _label = sample[\"label\"]\n",
    "        label_batch.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_batch.append(processed_text)\n",
    "    label_batch = torch.tensor(label_batch, dtype=torch.int64)\n",
    "    text_batch = pad_sequence(text_batch, batch_first=True, padding_value=vocab_[\"<pad>\"])\n",
    "    text_batch = torch.tensor(text_batch, dtype=torch.int64)\n",
    "    return text_batch.to(device), label_batch.to(device)\n",
    "\n",
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size,\n",
    "        model_type=\"lstm\",\n",
    "        use_pretrained_embedding=False,\n",
    "        embedding_dim=300,\n",
    "        hidden_dim=128,\n",
    "        num_layers=1,\n",
    "        output_dim=2,\n",
    "        bidirectional=True,\n",
    "        dropout=0.5\n",
    "        ):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        assert (model_type in [\"lstm\", \"gru\"]), \"rnn_type can be one of: 'lstm', 'gru'.\"\n",
    "        rnn_type = nn.LSTM if model_type == \"lstm\" else nn.GRU\n",
    "        if not use_pretrained_embedding:\n",
    "            self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab_[\"<pad>\"])\n",
    "        else:\n",
    "            pass\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn = rnn_type(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, input_batch):\n",
    "        # Pass the input batch into the embedding layer\n",
    "        embedded_input = self.embeddings(input_batch)\n",
    "        # Pass embedded batch into RNN\n",
    "        _, (hidden, _) = self.rnn(embedded_input)\n",
    "        # Use the last hidden state of the RNN as the output\n",
    "        # (might be 2 if use bidirectional)\n",
    "        if self.bidirectional:\n",
    "            hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]),dim=1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1,:,:]\n",
    "        # Pass the hidden state into the fully connected layer\n",
    "        output = self.fc(hidden_cat)\n",
    "        # NOTE: Do NOT pass the output through the softmax layer\n",
    "        # because CL loss expected un-normalized values\n",
    "        return output\n",
    "\n",
    "def train_one_epoch(epoch, model, optimizer, criterion, dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 200\n",
    "\n",
    "    for idx, (text_batch, label_batch) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text_batch)\n",
    "        loss = criterion(predicted_label, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label_batch).sum().item()\n",
    "        total_count += label_batch.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| training accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "    return model, loss\n",
    "\n",
    "def evaluate_one_epoch(epoch, model, dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for text, label in dataloader:\n",
    "            predicted_label = model(text)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    accu_val = total_acc/total_count\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} valid accuracy {:8.3f} '.format(epoch, accu_val))\n",
    "    print('-' * 59)\n",
    "    return accu_val\n",
    "\n",
    "# Code from https://towardsdatascience.com/lstm-text-classification-using-pytorch-2c6c657f8fc0\n",
    "def save_checkpoint(save_path, model, optimizer, valid_loss):\n",
    "    if save_path == None:\n",
    "        return\n",
    "\n",
    "    state_dict = {'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'valid_loss': valid_loss}\n",
    "\n",
    "    torch.save(state_dict, save_path)\n",
    "    print(f'Model saved to ==> {save_path}')\n",
    "\n",
    "\n",
    "def load_checkpoint(load_path, model, optimizer):\n",
    "    if load_path==None:\n",
    "        return\n",
    "\n",
    "    state_dict = torch.load(load_path, map_location=device)\n",
    "    print(f'Model loaded from <== {load_path}')\n",
    "\n",
    "    model.load_state_dict(state_dict['model_state_dict'])\n",
    "    optimizer.load_state_dict(state_dict['optimizer_state_dict'])\n",
    "    \n",
    "    return state_dict['valid_loss']\n",
    "\n",
    "def train(model, optimizer, criterion, train_dataloader, valid_dataloader, save_path, num_epochs=10):\n",
    "    best_valid_acc = 0.0\n",
    "    best_model = None\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        model, loss = train_one_epoch(epoch, model, optimizer, criterion, train_dataloader)\n",
    "        valid_acc = evaluate_one_epoch(epoch, model, valid_dataloader)\n",
    "        if valid_acc > best_valid_acc:\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_valid_acc = valid_acc\n",
    "            save_checkpoint(save_path, best_model, optimizer, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipykernel_launcher:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[4121,  381,    4,  ...,    1,    1,    1],\n",
       "         [  77,  670,   74,  ...,    1,    1,    1],\n",
       "         [  45,  458,  322,  ...,    1,    1,    1],\n",
       "         ...,\n",
       "         [  20, 1179,  480,  ...,    1,    1,    1],\n",
       "         [  13,    3,   72,  ...,   12, 4376,    2],\n",
       "         [  23, 2845, 1759,  ...,    1,    1,    1]]),\n",
       " tensor([0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "         0, 1, 0, 0, 0, 0, 1, 0]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(DataLoader(val_dataset, batch_size=32, shuffle=True, collate_fn=collate_batch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c_spino/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = RNNClassifier(len(vocab_), model_type=\"lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.state_dict of RNNClassifier(\n",
       "  (embeddings): Embedding(5235, 300, padding_idx=1)\n",
       "  (rnn): LSTM(300, 128, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  13,  118,    3,  ...,    1,    1,    1],\n",
      "        [2770,    4,    3,  ...,  299,  183,    2],\n",
      "        [  48,   10, 3476,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   3,   72, 1396,  ...,    1,    1,    1],\n",
      "        [   5, 1863,  289,  ...,    1,    1,    1],\n",
      "        [  60,   11,   30,  ...,    1,    1,    1]])\n",
      "torch.Size([32, 46])\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 1, 1, 0])\n",
      "torch.Size([32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipykernel_launcher:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "for i, elt in enumerate(val_iter):\n",
    "    x_batch, y_batch = elt[0], elt[1]\n",
    "    print(x_batch)\n",
    "    print(x_batch.shape)\n",
    "    print(y_batch)\n",
    "    print(y_batch.shape)\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 46, 300])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.Embedding(len(vocab_), 300, padding_idx=vocab_[\"<pad>\"])\n",
    "x_emb = emb(x_batch)\n",
    "x_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(300, 100, num_layers=2, bidirectional=True, batch_first=True)\n",
    "output, (hidden, cell) = lstm(x_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32, 100])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 200])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc = nn.Linear(200, 2)\n",
    "output = fc(hidden_cat)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipykernel_launcher:2: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.LogSoftmax(dim=1)\n",
    "output_s = softmax(output)\n",
    "output_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6984, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "loss(output_s, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = JsonDataset(\"/home/c_spino/comp_550/comp-550-project/data/rt-polaritydata/augmentation/validation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'like a skillful fisher , the director uses the last act to reel in the audience since its poignancy hooks us completely . ', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "for elt in val_dataset:\n",
    "    print(elt)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c_spino/anaconda3/envs/nlp/lib/python3.7/site-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
      "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29.5MB [00:02, 10.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.datasets_utils._RawTextIterableDataset at 0x7feb9d9df390>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "76563bcb032bc92047f8af45874c31363a00cf84e256bade488e530cde1f4b4b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('nlp': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
